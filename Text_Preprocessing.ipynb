{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.19.5\n",
        "!pip install nltk==3.2.5\n",
        "!pip install spacy==2.2.4"
      ],
      "metadata": {
        "id": "0qzhbQjhMJax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string"
      ],
      "metadata": {
        "id": "ol7JMR51Qskh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "WrawP7ERSck0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "##NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words_nltk = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZrOvB_GSmD1",
        "outputId": "8cb43e9c-4837-4b29-f536-97221e767267"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##SPACY \n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import spacy\n",
        "spacy_model = spacy.load('en_core_web_sm')\n",
        "\n",
        "stopwords_spacy = spacy_model.Defaults.stop_words"
      ],
      "metadata": {
        "id": "xz_lo-maTbUf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer= PorterStemmer()"
      ],
      "metadata": {
        "id": "b4BZMsjcUF3T"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "!python -m nltk.downloader wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS7Z-kg-U7ks",
        "outputId": "972d3aa6-4742-43da-a760-faee35927b79"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus='A technique known as text preprocessing is used to clean up text data before feeding it to a machine-learning model. Text data contains a variety of noise, such as emotions, punctuation, and text in a different capitalization. This is only the beginning of the difficulties we will face because machines cannot understand words, they require numbers. So we must find a fast and efficient way to transform text to numbers.'"
      ],
      "metadata": {
        "id": "2EhqYxMSQNH0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Lowercasing*"
      ],
      "metadata": {
        "id": "4vXI79XbQcwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = corpus.lower()\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvHlPnn0Qfz0",
        "outputId": "8ad0b5eb-1b10-448f-83ae-899eec1f06d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a technique known as text preprocessing is used to clean up text data before feeding it to a machine-learning model. text data contains a variety of noise, such as emotions, punctuation, and text in a different capitalization. this is only the beginning of the difficulties we will face because machines cannot understand words, they require numbers. so we must find a fast and efficient way to transform text to numbers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Removing digits*"
      ],
      "metadata": {
        "id": "SsYNCiXvQjcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus='BERT has been pre-trained on a vast corpus of unlabeled text, including the entire Wikipedia, which is 2,500 million words long, and various Book Corpus, which is over 800 million words long'\n",
        "corpus=re.sub(r'\\d+','',corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx52nzH7QrIH",
        "outputId": "79a877c3-7823-429d-c1dd-7f448278901b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT has been pre-trained on a vast corpus of unlabeled text, including the entire Wikipedia, which is , million words long, and various Book Corpus, which is over  million words long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Removing Punctuations*"
      ],
      "metadata": {
        "id": "up8TnZzFRCg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=corpus.translate(str.maketrans('','',string.punctuation))\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYxh0zfqRIlY",
        "outputId": "0a29c685-f99a-41b7-d427-904125aa149a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT has been pretrained on a vast corpus of unlabeled text including the entire Wikipedia which is  million words long and various Book Corpus which is over  million words long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Removing Trailing WhiteSpaces*"
      ],
      "metadata": {
        "id": "naozC21mR-Z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ' '.join([token for token in corpus.split()])\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BlrUythQSGSZ",
        "outputId": "f1424957-9c38-446c-cd0b-dbfea494fd7b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BERT has been pretrained on a vast corpus of unlabeled text including the entire Wikipedia which is million words long and various Book Corpus which is over million words long'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Tokenization*"
      ],
      "metadata": {
        "id": "yatZcX6RSWR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus_nltk = word_tokenize(corpus)\n",
        "print(\"\\nNLTK\\nTokenized corpus:\",tokenized_corpus_nltk)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8qrHf0LSZvN",
        "outputId": "9f687c16-fe08-4ee7-b893-d7075fc80dfe"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK\n",
            "Tokenized corpus: ['BERT', 'has', 'been', 'pretrained', 'on', 'a', 'vast', 'corpus', 'of', 'unlabeled', 'text', 'including', 'the', 'entire', 'Wikipedia', 'which', 'is', 'million', 'words', 'long', 'and', 'various', 'Book', 'Corpus', 'which', 'is', 'over', 'million', 'words', 'long']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSpacy:\")\n",
        "tokenized_corpus_spacy = word_tokenize(corpus)\n",
        "print(\"Tokenized Corpus:\",tokenized_corpus_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pL312beTgR_",
        "outputId": "87b8de8f-4da0-48b8-e078-c337bbb1eb02"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spacy:\n",
            "Tokenized Corpus: ['BERT', 'has', 'been', 'pretrained', 'on', 'a', 'vast', 'corpus', 'of', 'unlabeled', 'text', 'including', 'the', 'entire', 'Wikipedia', 'which', 'is', 'million', 'words', 'long', 'and', 'various', 'Book', 'Corpus', 'which', 'is', 'over', 'million', 'words', 'long']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Removing Stopwords*"
      ],
      "metadata": {
        "id": "h4dLEDdATnx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
        "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hTdwaybTVzm",
        "outputId": "47c8c2fd-bbf8-4da1-8972-0e067c02a3d4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized corpus without stopwords: ['BERT', 'pretrained', 'vast', 'corpus', 'unlabeled', 'text', 'including', 'entire', 'Wikipedia', 'million', 'words', 'long', 'various', 'Book', 'Corpus', 'million', 'words', 'long']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_without_sw= [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
        "\n",
        "print(\"Tokenized corpus without stopwords\",tokens_without_sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mx3TS83Tv8C",
        "outputId": "c0cd1326-a247-4d0a-fcb5-230505722962"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized corpus without stopwords ['BERT', 'pretrained', 'vast', 'corpus', 'unlabeled', 'text', 'including', 'entire', 'Wikipedia', 'million', 'words', 'long', 'Book', 'Corpus', 'million', 'words', 'long']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Difference between NLTK and spaCy output:\\n\",\n",
        "      set(tokenized_corpus_without_stopwords)-set(tokens_without_sw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zi4HbTKT59g",
        "outputId": "18db582b-79b2-451f-e7ea-294d2eabce89"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference between NLTK and spaCy output:\n",
            " {'various'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Stemming*"
      ],
      "metadata": {
        "id": "O6pAqR9aUAPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before Stemming: \")\n",
        "print(corpus)\n",
        "\n",
        "\n",
        "print('After Stemming with nltk tokens: ')\n",
        "for word in tokenized_corpus_nltk:\n",
        "  print(stemmer.stem(word),end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCrVFJoXUDTd",
        "outputId": "eadaee0d-f300-4f37-cb93-876e6fddcbb7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Stemming: \n",
            "BERT has been pretrained on a vast corpus of unlabeled text including the entire Wikipedia which is million words long and various Book Corpus which is over million words long\n",
            "After Stemming with nltk tokens: \n",
            "bert ha been pretrain on a vast corpu of unlabel text includ the entir wikipedia which is million word long and variou book corpu which is over million word long "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('After Stemming with spacy tokens: ')\n",
        "for word in tokenized_corpus_spacy:\n",
        "  print(stemmer.stem(word),end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWkDEIzcUmCD",
        "outputId": "1476615e-9ae4-4fc2-a258-77bf7f0457d1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Stemming with spacy tokens: \n",
            "bert ha been pretrain on a vast corpu of unlabel text includ the entir wikipedia which is million word long and variou book corpu which is over million word long "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Lemmatization*"
      ],
      "metadata": {
        "id": "zmzqnDhIU0q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZy6sHg0VWcJ",
        "outputId": "5fac7bcb-2b2d-4a67-f516-8c2c8c74318d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = spacy.load('en_core_web_sm', disable = ['parser','ner'])\n",
        "\n",
        "doc = load_model(corpus)\n",
        "\" \".join([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "10ZZXmVqU4Ul",
        "outputId": "9bb90721-b72f-4bd5-b8c1-636ce0892bfa"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BERT have be pretraine on a vast corpus of unlabeled text include the entire Wikipedia which be million word long and various Book Corpus which be over million word long'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *POS Tagging*"
      ],
      "metadata": {
        "id": "ppc30z6aZOX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"POS Tagging using spacy:\")\n",
        "doc = spacy_model(corpus)\n",
        "# Token and Tag\n",
        "for token in doc:\n",
        "    print(token,\":\", token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlj__eCNZRHC",
        "outputId": "c8361ae0-803f-4235-899c-f5bbb31ce3b3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging using spacy:\n",
            "BERT : PROPN\n",
            "has : AUX\n",
            "been : AUX\n",
            "pretrained : VERB\n",
            "on : ADP\n",
            "a : DET\n",
            "vast : ADJ\n",
            "corpus : NOUN\n",
            "of : ADP\n",
            "unlabeled : ADJ\n",
            "text : NOUN\n",
            "including : VERB\n",
            "the : DET\n",
            "entire : ADJ\n",
            "Wikipedia : PROPN\n",
            "which : DET\n",
            "is : AUX\n",
            "million : NUM\n",
            "words : NOUN\n",
            "long : ADJ\n",
            "and : CCONJ\n",
            "various : ADJ\n",
            "Book : PROPN\n",
            "Corpus : PROPN\n",
            "which : DET\n",
            "is : AUX\n",
            "over : ADP\n",
            "million : NUM\n",
            "words : NOUN\n",
            "long : ADV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pos tagging using nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(\"POS Tagging using NLTK:\")\n",
        "pprint(nltk.pos_tag(tokenized_corpus_nltk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBIjOw8AZiLk",
        "outputId": "f735c72b-e02f-4bb4-c87c-f0aa455267ee"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "POS Tagging using NLTK:\n",
            "[('BERT', 'NNP'),\n",
            " ('has', 'VBZ'),\n",
            " ('been', 'VBN'),\n",
            " ('pretrained', 'VBN'),\n",
            " ('on', 'IN'),\n",
            " ('a', 'DT'),\n",
            " ('vast', 'JJ'),\n",
            " ('corpus', 'NN'),\n",
            " ('of', 'IN'),\n",
            " ('unlabeled', 'JJ'),\n",
            " ('text', 'NN'),\n",
            " ('including', 'VBG'),\n",
            " ('the', 'DT'),\n",
            " ('entire', 'JJ'),\n",
            " ('Wikipedia', 'NNP'),\n",
            " ('which', 'WDT'),\n",
            " ('is', 'VBZ'),\n",
            " ('million', 'CD'),\n",
            " ('words', 'NNS'),\n",
            " ('long', 'JJ'),\n",
            " ('and', 'CC'),\n",
            " ('various', 'JJ'),\n",
            " ('Book', 'NNP'),\n",
            " ('Corpus', 'NNP'),\n",
            " ('which', 'WDT'),\n",
            " ('is', 'VBZ'),\n",
            " ('over', 'IN'),\n",
            " ('million', 'CD'),\n",
            " ('words', 'NNS'),\n",
            " ('long', 'JJ')]\n"
          ]
        }
      ]
    }
  ]
}